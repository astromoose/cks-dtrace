== Our DTrace scripts

These are DTrace scripts that we've found useful to troubleshoot things
in our Solaris + ZFS + iSCSI fileserver environment; you can find some
details of that environment at

    http://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSFileserverSetup

These scripts are for Solaris 10 update 8 plus some patches and may not
work on any later or earlier version. They work for us but as usual
there are no warranties. I make no claims that they are general; we
wrote them to find out the information that we needed at the time.

=== Using the scripts

There are four different things you can use these scripts for, depending
on what you need.

nfs3-clients.d and zfsstats.d provide top-like periodic reports on NFS
v3 server activity and ZFS activity, suitable for running either when
there's a load problem or you just want to know generally what sort of
things are going on. nfs3-clients.d is focused on the sources of client
activity so you can zero in on where load is coming from. zfsstats.d
focuses on what pools are active and how.

nfs3-dtrace.d and iscsi-trace.d gather aggregate timing and activity
information over however long you run them for NFS v3 server and iSCSI
initiator activity. Their goal is to characterize how your IO looks like
and how things seem to be performing; what activities are fast or slow,
and how things break down. These two scripts were developed first and
are the most primitive and peculiar.

nfs3-long.d and iscsi-long.d provide immediate reports of long operations
at the NFS v3 server or iSCSI initiator level. You would run these if you
think (or know) that you have unusually slow operations and want to know
what they look like and how often they happen.

zfstrace.d reports every ZFS, ZIO, and iSCSI initiator operation, with
duration and other information. This isn't the kind of thing you read
directly, live; instead it's the kind of thing you feed to detailed
analysis that needs full information about each event. For example,
working out your 99% performance point or graphing the distribution
of IO times.

=== Script details

The current DTrace scripts here:

- nfs3-dtrace.d: gathers timing information on NFS v3 server performance
  and ZFS performance (both high level and low level). This is mostly
  presented as distributions.

  - rfs3_* operations are NFS v3 server operations.
  - zfs_read and zfs_write are the high-level interface into ZFS, used
    by eg the NFS v3 server. They can hit the ARC instead of doing real
    IO.
  - zio_* operations are low level ZFS ZIO IO operations; as far as I
    know they come after the ARC. I don't really understand what zio_free
    does. In our environment, zio_ioctl operations are only used for
    sending disk flushes to the disks and this script ignores them for
    reasons beyond the scope of this README.

- iscsi-trace.d: gathers relatively low-level timing information on
  iSCSI initiator performance. This accumulates not just averages
  (which iostat will sort of tell you) but also distributions and
  breaks down the numbers in various ways.

- nfs3-long.d: report on long NFS v3 read, write, and commit operations.
  We don't report long operations for other sorts of things because right
  now we don't have problems with those (or at least we don't think we do);
  reads and writes are the bulk of our IO and the bulk of our problems.

- iscsi-long.d: report on long iSCSI SCSI operations.

- nfs3-clients.d: provides an every-10-seconds report of NFS v3 activity,
  primarily of read and write activity. The activity is broken down in
  various ways so that we/you can see who is causing it. See the comments
  at the start of the script for some information on what it reports.

- zfsstats.d: provides an every-10-seconds report of ZFS activity, again
  primarily of read and write activity. See the comments at the start of
  the script for information on what the reports mean.

- zfstrace.d: provide timing traces of zfs_read()/zfs_write(), ZIO
  operations, and iSCSI operations; every operation is reported with
  the time it took and various other details. The output is cryptic
  and compact due to volume and speed concerns.

Understanding some of the details of this information requires some
knowledge of (Open)Solaris kernel internals, but I think that much of it
will be relatively obvious.

Right now we ignore the IO sizes and make no attempt to, eg, normalize
the operation durations by the amount of data they were working on. In
our environment this works out fine, but it may not in yours.

Note that one zfs_read() or zfs_write() will generally result in
multiple zio_* operations. Some of these operations are synchronous and
some are not (eg, readahead or writeback). nfs3-dtrace.d makes some
attempt to only really track synchronous operations that delay the
completion of the higher-level IO but is probably not perfect about it.

ZIO operations seem to bubble down (and up) through the layers of vdevs
involved in any particular ZFS pool. We currently make no attempt to
only follow ZIO operations directed against physical disks, so it's
routine for the count of ZIO operations to vastly exceeded the count of
zfs_read() and zfs_write() calls.

BUGS: some of these scripts are terribly named.

(Chris Siebenmann, October 22 2012)
