== Our DTrace scripts

These are DTrace scripts that we've found useful to troubleshoot things
in our Solaris + ZFS + iSCSI fileserver environment; you can find some
details of that environment at

    http://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSFileserverSetup

These scripts are for Solaris 10 update 8 plus some patches and may not
work on any later or earlier version. They work for us but as usual
there are no warranties. I make no claims that they are general; we
wrote them to find out the information that we needed at the time.

The current DTrace scripts here:

- nfs3-dtrace.d: gathers timing information on NFS v3 server performance
  and ZFS performance (both high level and low level). This is mostly
  presented as distributions.

  - rfs3_* operations are NFS v3 server operations.
  - zfs_read and zfs_write are the high-level interface into ZFS, used
    by eg the NFS v3 server. They can hit the ARC instead of doing real
    IO.
  - zio_* operations are low level ZFS ZIO IO operations; as far as I
    know they come after the ARC. I don't really understand what zio_free
    does. In our environment, zio_ioctl operations are only used for
    sending disk flushes to the disks and this script ignores them for
    reasons beyond the scope of this README.

- iscsi-trace.d: gathers relatively low-level timing information on
  iSCSI initiator performance. This accumulates not just averages
  (which iostat will sort of tell you) but also distributions and
  breaks down the numbers in various ways. With sufficient verbosity
  it can log specifics about long IO operations.

Understanding some of the details of this information requires some
knowledge of (Open)Solaris kernel internals, but I think that much of it
will be relatively obvious.

Right now we ignore the IO sizes and make no attempt to, eg, normalize
the operation durations by the amount of data they were working on. In
our environment this works out fine, but it may not in yours.

Note that one zfs_read() or zfs_write() will generally result in
multiple zio_* operations. Some of these operations are synchronous and
some are not (eg, readahead or writeback). nfs3-dtrace.d makes some
attempt to only really track synchronous operations that delay the
completion of the higher-level IO but is probably not perfect about it.

ZIO operations seem to bubble down (and up) through the layers of vdevs
involved in any particular ZFS pool. We currently make no attempt to
only follow ZIO operations directed against physical disks, so it's
routine for the count of ZIO operations to vastly exceeded the count of
zfs_read() and zfs_write() calls.
